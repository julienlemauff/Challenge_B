---
title: "Challenge B"
author: "Le Mauff Julien and Guelimi Mohamed"
date: "07/11/2017"
output: pdf_document
---
The link of Github deposit: https://github.com/julienlemauff

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
library(tidyverse)
library(caret)
library(randomForest)
library(latexpdf)
library(readr)
library(knitr)
library(ggplot2)
library(np)
library(readxl)
```

## Task 1B - Predicting house prices in Ames, Iowa

```{r import dataset, echo = FALSE, results='hide'}
train <- read.csv(file = "/Users/lemauffjulien/rprog1/Challenge A/train.csv")
attach(train)
```


Step 1 - Choose a ML technique : non-parametric kernel estimation, random forests, etc... Give a brief intuition of how it works.

We choose to use Random Forest which is usefull when we have an important number of explanatory variables. It is a machine learning algorithm which is very usefull to spot the links between the dependant variable and the explanatory ones: Random Forest class them according to their links with the dependant variable. It gonna select random sample of a dataset. If we want to go futher, it follows the principle of "bagging" which consists to compute the mean of the predictions of several independants models to reduce the prediction error.

Step 2 - Train the chosen technique on the training data. Hint : packages np for non-parametric regressions, randomForest for random forests. Donâ€™t use the variable Id as a feature.

Here we clean the data train as it is done in the Challenge A's correction  

```{r missing data step 1, echo = FALSE, results='hide'}
head(train)
train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)
#summarize the data set using the function sum(is.na()); is.na(.)
```

Checking the first rows of the data indicates that there are columns which have missing values. There are some variables with a lot of missing observations, and some other that have only some missing observations. We remove the variables that have more than 100 missing observations

```{r missing data step 2, echo = FALSE}
remove.vars <- train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 100) %>% select(feature) %>% unlist

train <- train %>% select(- one_of(remove.vars))
```

For the rest of the variables with missing values, we remove the observations with the missing values

```{r missing data step 3, echo = FALSE, results='hide'}
train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)

train <- train %>% filter(is.na(GarageType) == FALSE, is.na(MasVnrType) == FALSE, is.na(BsmtFinType2) == FALSE, is.na(BsmtExposure) == FALSE, is.na(Electrical) == FALSE) # to make sure that it is clear
```

Now we can train the chosen technique on the training data. It gives us that 87 percent of the variance is explained.

```{r random forest regression, echo = TRUE}
fit <- randomForest(SalePrice ~ . -Id, data = train, ntree = 500, 
                    mtry = 24 , na.action = na.roughfix)
print(fit)
```

Step 3 - Make predictions on the test data, and compare them to the predictions of a linear regression of your choice.

```{r import the data test, echo = FALSE, results = 'hide'}
data_test <- read.csv(file = "/Users/lemauffjulien/rprog1/Challenge A/test.csv") # import the data test.csv
attach(data_test)
common <- intersect(names(train), names(data_test))
for (p in common) {
  if (class(train [[p]]) == "factor") {
    levels(data_test[[p]]) <- levels(train[[p]])
  }
}            # This is because train & test doesn't have the same numbers of row 
```

Since now train & test have the same number of row we can do the predictions 

```{r predictions, echo = FALSE, results = 'hide'}
# take a model of my choice
lm_model_2 <- lm(SalePrice ~ MSZoning + LotArea + Neighborhood  + YearBuilt + OverallQual, data = train)
summary(lm_model_2)

# Use the model that I just chose to make predictions for the test set
prediction <- data.frame(Id = data_test$Id, SalePrice_predict = predict(lm_model_2, data_test, type="response"))
prediction

# Make the prediction with the model that I found with the randomforest regression
prediction_bis <- data.frame(Id = data_test$Id, SalePrice_predict = predict(fit, data_test, type="response"))
colnames(prediction_bis) <- c("Id", "SalePrice_fitpredict")
prediction_bis
```

Which of these two predictions are the best ? We note that the model "fit" explain 87% of the variance whereas "lm_model_2" has a multiple R-squared of 0,7253. 


## Task 2B - Overfitting in Machine Learning (continued)

Step 1 - Estimate a low-flexibility local linear model on the training data.

In this exercise, we used what was done in the correction of Challenge A to avoid mistakes, even if in some cases we could have done otherwise

```{r simulation, echo = FALSE, results = 'hide'}
# Simulating an overfit
# True model : y = x^3 + epsilon
set.seed(1)
Nsim <- 150
b <- c(0,1)
x0 <- rep(1, Nsim)
x1 <- rnorm(n = Nsim)

X <- cbind(x0, x1^3)
y.true <- X %*% b

eps <- rnorm(n = Nsim)
y <- X %*% b + eps

df <- tbl_df(y[,1]) %>% rename(y = value) %>% bind_cols(tbl_df(x1)) %>% rename(x = value) %>% bind_cols(tbl_df(y.true[,1])) %>% rename(y.true = value)

#We put X (the random variable) and the output y in a table using the function data.frame
simulation <- data.frame(X,y)
simulation
```

Henceforth we can create the 2 dataset : training and test

```{r split, echo = TRUE, results = 'hide'}
# We split the sample into two subsample 
training.index <- createDataPartition(y = y, times = 1, p = 0.8)
df <- df %>% mutate(which.data = ifelse(1:n() %in% training.index$Resample1, "training", "test"))

training <- df %>% filter(which.data == "training")
test <- df %>% filter(which.data == "test")
```

Everything in ready to do question 1 & 2

```{r sol step 1, echo = TRUE}
# Train local linear model y ~ x on training, using default low flexibility (high bandwidth):
ll.fit.lowflex <- npreg(y ~ x, data = training, method = "11", bws = 0.5)
summary(ll.fit.lowflex)
```

Step 2 - Estimate a high-flexibility local linear model on the training data.

```{r sol step 2, echo = TRUE}
# Train local linear model y ~ x on training, using default low flexibility 
ll.fit.highflex <- npreg(y ~ x, data = training, method = "ll", bws = 0.01)
summary(ll.fit.highflex)
```

Step 3 - Plot the scatterplot of x-y, along with the predictions of ll.fit.lowflex and ll.fit.highflex, on only the training data.

```{r sol step 3, echo = FALSE}
df <- df %>% mutate(y.ll.lowflex = predict(object = ll.fit.lowflex, newdata = df), y.ll.highflex = predict(object = ll.fit.highflex, newdata = df))
training <- training %>% mutate(y.ll.lowflex = predict(object = ll.fit.lowflex, newdata = training), y.ll.highflex = predict(object = ll.fit.highflex, newdata = training))

ggplot(training) + geom_point(mapping = aes(x = x, y = y)) + 
  geom_line(mapping = aes(x = x, y = y.true)) + 
  geom_line(mapping = aes(x = x, y = y.ll.lowflex), color = "red") + 
  geom_line(mapping = aes(x = x, y = y.ll.highflex), color = "green")
```  

Step 4 - Between the two models, which predictions are more variable? Which predictions have the least bias?

We can see (in green) that the highflex model tends to follow all the points (the reals observation), so this model gives at the same time the best predictions but also the predictions that are the more variables. We also know that the estimator's variance is in link the variance of observation, that's why the predictions with the least bias is lowflex.

Step 5 - Plot the scatterplot of x-y, along with the predictions of ll.fit.lowflex and ll.fit.highflex now using the test data. Which predictions are more variable? What happened to the bias of the least biased model?

```{r sol step 5, echo = FALSE}
ll.fit.lowflex <- npreg(y ~ x, data = test, method = "11", bws = 0.5)
ll.fit.highflex <- npreg(y ~ x, data = test, method = "ll", bws = 0.01)

df <- df %>% mutate(y.ll.lowflex = predict(object = ll.fit.lowflex, newdata = df), y.ll.highflex = predict(object = ll.fit.highflex, newdata = df))
test <- test %>% mutate(y.ll.lowflex = predict(object = ll.fit.lowflex, newdata = test), y.ll.highflex = predict(object = ll.fit.highflex, newdata = test))

ggplot(test) + geom_point(mapping = aes(x = x, y = y)) + 
  geom_line(mapping = aes(x = x, y = y.true)) + 
  geom_line(mapping = aes(x = x, y = y.ll.lowflex), color = "red") + 
  geom_line(mapping = aes(x = x, y = y.ll.highflex), color = "green")
```

Here the highflex model gives almost always the good prediction that's why this model have the predictions the more variable. Since there are less observations, the bias of the least biased model is reduced.

Step 6 - Create a vector of bandwidth going from 0.01 to 0.5 with a step of 0.001

```{r sol step 6, echo = FALSE, results='hide'}
# Create vector of several bandwidth
bw <- seq(0.01, 0.5, by = 0.001)
```

Step 7 - Estimate a local linear model y ~ x on the training data with each bandwidth.

```{r sol step 7, echo = FALSE, results='hide'}
# Train local linear model y ~ x on training with each bandwidth
llbw.fit <- lapply(X = bw, FUN = function(bw) {npreg(y ~ x, data = training, method = "ll", bws = bw)})
```

Step 8 - Compute for each bandwidth the MSE on the training data.

```{r sol step 8, echo = FALSE, results='hide'}
# Compute for each bandwidth the MSE-training
mse.training <- function(fit.model){
  predictions <- predict(object = fit.model, newdata = training)
  training %>% mutate(squared.error = (y - predictions)^2) %>% summarize(mse = mean(squared.error))
}
mse.train.results <- unlist(lapply(X = llbw.fit, FUN = mse.training))
```

Step 9 - Compute for each bandwidth the MSE on the test data.

```{r sol step 9, echo = FALSE, results='hide'}
# Compute for each bandwidth the MSE-test
mse.test <- function(fit.model){
  predictions <- predict(object = fit.model, newdata = test)
  test %>% mutate(squared.error = (y - predictions)^2) %>% summarize(mse = mean(squared.error))
}
mse.test.results <- unlist(lapply(X = llbw.fit, FUN = mse.test))
```

Step 10 - Draw on the same plot how the MSE on training data, and test data, change when the bandwidth increases. Conclude.

```{r sol step 10, echo = FALSE}
# Plot
mse.df <- tbl_df(data.frame(bandwidth = bw, mse.train = mse.train.results, mse.test = mse.test.results))
ggplot(mse.df) + 
  geom_line(mapping = aes(x = bandwidth, y = mse.train), color = "blue") +
  geom_line(mapping = aes(x = bandwidth, y = mse.test), color = "orange")
```

When bandwidth tends to 0, mse.train tends to 0 whereas mse.test tends to plus infintiy. But when bandwidth tends to 1, boths follows the same trends. Moreover mse.train is bigger than mse.test from bandwidth = 0,13. So depending on the value of bandwidth, we will choose either mse.training or mse.test.


##Task 3B

Step 1 - Import the CNIL dataset from the Open Data Portal.

```{r import CNIL, echo=FALSE, results='hide', eval=FALSE}
data_CNIL <- read_excel("~/rprog1/challenge B/CNIL.xlsx")
View(CNIL)
View(data_CNIL)
head(data_CNIL)
attach(data_CNIL)
```

Step 2 - Show a (nice) table with the number of organizations that has nominated a CNIL per department.

As in Task1B, we first remove the missing values (NAs) to "clean" the dataset


Then we can show in a nice table the number of organizations that has nominated a CNIL per department.

```{r the nice table, echo = TRUE, results = 'hold', eval = FALSE}
# I create the feature 'department' which correspond to the firt two number of the feature 'Code Postal'
library(stringr)
# I need this package to use the function 'str_sub'
department <- str_sub(`Code Postal`, start=1, end=2)
Department <- as.numeric(department)
nicetable <- table(Department)

# Now I recall the columns
nicetable <- data.frame(nicetable)
colnames(nicetable) <- c("Departments", "Numbers_of_organization")
kable(nicetable)
```

Step 3 








